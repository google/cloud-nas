# Copyright 2021 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# MNasNet Imagenet search config to test proxy-task.
# The batch size is set for 2 V100 GPUs.
# Set total training steps to be 50.
# Run validation after every 2 steps.
# Use train data as: gs://cloud-nas-public-eu/classification/imagenet/train-009*
# Use validation data as: gs://cloud-nas-public-eu/classification/imagenet/validation-0009?-of-00128
# which has (20000/128)*10 = ~1562 examples.
runtime:
  distribution_strategy: 'mirrored'
  mixed_precision_dtype: 'float16'
  loss_scale: 'dynamic'
task:
  model:
    # Note that `num_classes` is the total number of classes including
    # one background class with an index value of 0.
    num_classes: 1001
    # Controls the effective image-size for training.
    input_size: [224, 224, 3]
    backbone:
      type: 'tunable_mnasnet'
      tunable_mnasnet:
        # The number of filters of the output endpoints.
        endpoints_num_filters: 256
    dropout_rate: 0.2
    norm_activation:
      norm_epsilon: 0.001
      norm_momentum: 0.99
      use_sync_bn: true
  losses:
    one_hot: true
    label_smoothing: 0.1
  train_data:
    # Reduce the batch-size if you run into out-of-memory error.
    # Out-of-memory error may also be resolved by reducing
    # the image-size but at the expense of quality.
    # The global batch size for all included GPUs,
    # batch-size-per-gpu = global_batch_size / num-gpus, under mirrored distribution strategy.
    global_batch_size: 512
    dtype: 'float16'
  validation_data:
    # NOTE: Partial batch will not be used for evaluation. So ideally,
    # the total-number of eval-samples should be divisible by
    # 'eval_batch_size' if you want all the samples to be used.
    global_batch_size: 512
    dtype: 'float16'
    drop_remainder: false

  # Backbone is trained from scratch.
  init_checkpoint: null
  init_checkpoint_modules: null
trainer:
  train_steps: 50
  validation_steps: 3 # 1562/512
  steps_per_loop: 2
  validation_interval: 2
  summary_interval: 2
  checkpoint_interval: 2
  optimizer_config:
    optimizer:
      type: 'sgd'
      sgd:
        momentum: 0.9
    learning_rate:
      stepwise:
        boundaries: [10008]  # Decay at 4th epoches.
        values: [0.032, 0.0032]
      type: 'stepwise'
    warmup:
      type: 'linear'
      linear:
        warmup_steps: 2

